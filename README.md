# ğŸ’¬ Lokaler KI-Chat (Flask + Ollama)

Ein vollstÃ¤ndig **lokaler KI-Chatbot**, der **Ã¼ber die eigene Grafikkarte (GPU)** lÃ¤uft â€“ kein API-Key, keine Cloud.  
Er kombiniert eine minimalistische WeboberflÃ¤che mit **Ollama** als Backend, um groÃŸe Sprachmodelle wie **LLaMA 3** oder **Mistral** direkt **offline auf dem eigenen Rechner** auszufÃ¼hren.

---
## ğŸ–¥ï¸ Demo

**Darkmode**

![Screenshot der Chat-UI](assets/screenshot_ui.png)

---

**Lightmode + Modellauswahl**

![Screenshot der Chat-UI (Whitemode)](assets/screenshot_ui_whitemode.png)

---

## ğŸš€ Funktionen

- ğŸ§  **Chat mit lokalem KI-Modell (Ollama)**
- âš¡ **LÃ¤uft vollstÃ¤ndig auf der eigenen GPU / CPU**  
  â†’ keine Internetverbindung oder Cloud-AbhÃ¤ngigkeit
- ğŸ“„ **Datei-Upload (PDF, TXT, DOCX)** mit automatischer Textextraktion  
- ğŸ’¬ **Echtzeit-Streaming** der Antworten (Zeichenweise wie bei ChatGPT)  
- ğŸŒ“ **Dark-/Lightmode** mit Themespeicherung  
- ğŸ§± **100 % lokal & datensicher**
- ğŸ” **Markdown-UnterstÃ¼tzung** (CodeblÃ¶cke, Listen, Formatierungen)

---

## ğŸ› ï¸ Tech Stack

| Bereich | Technologien |
|----------|---------------|
| **Frontend** | HTML, CSS, Vanilla JS, marked.js, Material Icons |
| **Backend** | Python, Flask, Flask-CORS, Requests, PyPDF2 |
| **KI-Engine** | Ollama (lokal auf GPU/CPU) â€“ z. B. LLaMA 3, Mistral, Gemma 2 |
---

## âš™ï¸ Installation & Setup

1. **Ollama installieren** â†’ https://ollama.com/download  

2. **Modell laden** (Beispiel):
   ```bash
   ollama run gpt-oss:20b

Bibliothek der Modelle: https://ollama.com/library

3. **Projekt starten**

# Repository klonen
git clone https://github.com/<dein-github-name>/lokaler-ki-chat.git
cd lokaler-ki-chat

# AbhÃ¤ngigkeiten
pip install flask flask-cors requests PyPDF2

# Backend starten
python server.py

# Benutzen
index.html im Browser Ã¶ffnen
