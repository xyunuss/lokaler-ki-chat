# üíª OllamaDesk ‚Äì Chatbot √ºber eigene GPU (Flask + Ollama)

Ein vollst√§ndig **lokaler KI-Chatbot**, der **√ºber die eigene Grafikkarte (GPU)** l√§uft ‚Äì kein API-Key, keine Cloud.  
Er kombiniert eine minimalistische Weboberfl√§che mit **Ollama** als Backend, um gro√üe Sprachmodelle wie **LLaMA 3** oder **Mistral** direkt **offline auf dem eigenen Rechner** auszuf√ºhren.

---
## üñ•Ô∏è Demo

**Darkmode**

![Screenshot der Chat-UI](assets/screenshot_ui.png)

---

**Lightmode + Modellauswahl**

![Screenshot der Chat-UI (Whitemode)](assets/screenshot_ui_whitemode.png)

---

## üöÄ Funktionen

-  **Chat mit lokalem KI-Modell (Ollama)**
-  **L√§uft vollst√§ndig auf der eigenen GPU / CPU**  
  ‚Üí keine Internetverbindung oder Cloud-Abh√§ngigkeit
-  **PDF Datei-Upload** mit automatischer Textextraktion  
-  **Echtzeit-Streaming** der Antworten (Zeichenweise wie bei ChatGPT)  
-  **Dark-/Lightmode** mit Themespeicherung  
-  **100 % lokal & datensicher**
-  **Markdown-Unterst√ºtzung** (Codebl√∂cke, Listen, Formatierungen)

---

## üõ†Ô∏è Tech Stack

| Bereich | Technologien |
|----------|---------------|
| **Frontend** | HTML, CSS, Vanilla JS, marked.js, Material Icons |
| **Backend** | Python, Flask, Flask-CORS, Requests, PyPDF2 |
| **KI-Engine** | Ollama (lokal auf GPU/CPU) ‚Äì z. B. LLaMA 3, deepseek-coder, gpt-oss |
---

## ‚öôÔ∏è Installation & Setup

1. **Ollama installieren** ‚Üí https://ollama.com/download  

2. **Modell laden** (Beispiel):
   ```bash
   ollama run gpt-oss:20b

   # Bibliothek f√ºr weitere Modelle: https://ollama.com/library

   # Modelle anzeigen lassen mit ollama list
NAME                        ID              SIZE      MODIFIED
deepseek-coder:6.7b         ce298d984115    3.8 GB    4 hours ago
gpt-oss:latest              17052f91a42e    13 GB     4 hours ago
qwen2.5-coder:14b           9ec8897f747e    9.0 GB    4 hours ago
mistral:7b                  6577803aa9a0    4.4 GB    4 hours ago
gpt-oss:20b                 17052f91a42e    13 GB     5 hours ago
qwq:latest                  009cb3f08d74    19 GB     26 hours ago
llama2-uncensored:latest    44040b922233    3.8 GB    28 hours ago
llama3.1:latest             46e0c10c039e    4.9 GB    29 hours ago




3. **Projekt starten**
   ```bash
    # Repository klonen
    git clone https://github.com/xyunuss/local-ai-chat

    # Abh√§ngigkeiten
    pip install flask flask-cors requests PyPDF2

    # Backend starten
    python server.py

    # Benutzen
    index.html im Browser √∂ffnen

---

## üìú Lizenz

Ver√∂ffentlicht unter der MIT License
¬© 2025 Yunus Yakup Peter Schultze