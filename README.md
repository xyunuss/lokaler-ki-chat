# üí¨ Lokaler KI-Chat (Flask + Ollama)

Ein vollst√§ndig **lokaler KI-Chatbot**, der **√ºber die eigene Grafikkarte (GPU)** l√§uft ‚Äì kein API-Key, keine Cloud.  
Er kombiniert eine minimalistische Weboberfl√§che mit **Ollama** als Backend, um gro√üe Sprachmodelle wie **LLaMA 3** oder **Mistral** direkt **offline auf dem eigenen Rechner** auszuf√ºhren.

---
## üñ•Ô∏è Demo

**Darkmode**

![Screenshot der Chat-UI](assets/screenshot_ui.png)

---

**Lightmode + Modellauswahl**

![Screenshot der Chat-UI (Whitemode)](assets/screenshot_ui_whitemode.png)

---

## üöÄ Funktionen

-  **Chat mit lokalem KI-Modell (Ollama)**
-  **L√§uft vollst√§ndig auf der eigenen GPU / CPU**  
  ‚Üí keine Internetverbindung oder Cloud-Abh√§ngigkeit
-  **PDF Datei-Upload** mit automatischer Textextraktion  
-  **Echtzeit-Streaming** der Antworten (Zeichenweise wie bei ChatGPT)  
-  **Dark-/Lightmode** mit Themespeicherung  
-  **100 % lokal & datensicher**
-  **Markdown-Unterst√ºtzung** (Codebl√∂cke, Listen, Formatierungen)

---

## üõ†Ô∏è Tech Stack

| Bereich | Technologien |
|----------|---------------|
| **Frontend** | HTML, CSS, Vanilla JS, marked.js, Material Icons |
| **Backend** | Python, Flask, Flask-CORS, Requests, PyPDF2 |
| **KI-Engine** | Ollama (lokal auf GPU/CPU) ‚Äì z. B. LLaMA 3, deepseek-coder, gpt-oss |
---

## ‚öôÔ∏è Installation & Setup

1. **Ollama installieren** ‚Üí https://ollama.com/download  

2. **Modell laden** (Beispiel):
   ```bash
   ollama run gpt-oss:20b

Bibliothek f√ºr weitere Modelle: https://ollama.com/library

3. **Projekt starten**
   ```bash
    # Repository klonen
    git clone https://github.com/xyunuss/local-ai-chat

    # Abh√§ngigkeiten
    pip install flask flask-cors requests PyPDF2

    # Backend starten
    python server.py

    # Benutzen
    index.html im Browser √∂ffnen

