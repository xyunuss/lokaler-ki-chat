# ğŸ’» OllamaDesk â€“ Chatbot Ã¼ber eigene GPU (Flask + Ollama)

Ein vollstÃ¤ndig **lokaler KI-Chatbot**, der **Ã¼ber die eigene Grafikkarte (GPU)** lÃ¤uft â€“ kein API-Key, keine Cloud.  
Er kombiniert eine minimalistische WeboberflÃ¤che mit **Ollama** als Backend, um groÃŸe Sprachmodelle wie **LLaMA 3**, **Mistral** oder **gpt-oss** direkt **offline auf dem eigenen Rechner** auszufÃ¼hren.

---
## ğŸ–¥ï¸ Demo

**Darkmode**

![Screenshot der Chat-UI](assets/screenshot_ui.png)

---

**Lightmode + Modellauswahl**

![Screenshot der Chat-UI (Whitemode)](assets/screenshot_ui_whitemode.png)

---

## ğŸš€ Funktionen

-  **Chat mit lokalem KI-Modell (Ollama)**
-  **LÃ¤uft vollstÃ¤ndig auf der eigenen GPU / CPU**  
  â†’ keine Internetverbindung oder Cloud-AbhÃ¤ngigkeit
-  **PDF Datei-Upload** mit automatischer Textextraktion  
-  **Echtzeit-Streaming** der Antworten (Zeichenweise wie bei ChatGPT)  
-  **Dark-/Lightmode** mit Themespeicherung  
-  **100 % lokal & datensicher**
-  **Markdown-UnterstÃ¼tzung** (CodeblÃ¶cke, Listen, Formatierungen)

---

## ğŸ› ï¸ Tech Stack

| Bereich | Technologien |
|----------|---------------|
| **Frontend** | HTML, CSS, Vanilla JS, marked.js, Material Icons |
| **Backend** | Python, Flask, Flask-CORS, Requests, PyPDF2 |
| **KI-Engine** | Ollama (lokal auf GPU/CPU) â€“ z. B. LLaMA 3, deepseek-coder, gpt-oss |
---

## âš™ï¸ Installation & Setup

1. **Ollama installieren** â†’ https://ollama.com/download  

2. **Modell laden** (Beispiel):
   ```bash
   ollama run gpt-oss:20b

   # Modelle anzeigen lassen mit ollama list

Bibliothek fÃ¼r weitere Modelle: https://ollama.com/library



3. **Projekt starten**
   ```bash
    # Repository klonen
    git clone https://github.com/xyunuss/local-ai-chat

    # AbhÃ¤ngigkeiten
    pip install flask flask-cors requests PyPDF2

    # Backend starten
    python server.py

    # Benutzen
    index.html im Browser Ã¶ffnen

---

## ğŸ“œ Lizenz

VerÃ¶ffentlicht unter der MIT License
Â© 2025 Yunus Yakup Peter Schultze