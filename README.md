# ğŸ’¬ Lokaler KI-Chat (Flask + Ollama)

Ein vollstÃ¤ndig lokaler KI-Chatbot mit Datei-Upload, Markdown-Rendering, Theme-Umschaltung und Chatverlauf.  
Er nutzt **Ollama** als Backend, um LLMs lokal laufen zu lassen â€“ **ohne API-Keys oder Cloud**.

---

## ğŸš€ Funktionen
- ğŸ§  Chat mit lokalem KI-Modell (Ollama)
- ğŸ“„ Datei-Upload (PDF, TXT, DOCX) â†’ automatische Analyse
- ğŸ’¬ Live-Streaming der Antworten
- ğŸŒ“ Dark-/Lightmode + Themespeicherung
- ğŸ—‚ï¸ Chat-Verlauf (Client-Session)
- ğŸ§± 100 % lokal

---

## ğŸ› ï¸ Tech Stack
| Bereich | Technologie |
|---|---|
| Backend | Python, Flask, PyPDF2, Requests |
| Frontend | HTML, Vanilla JS, CSS, marked.js |
| KI-Engine | Ollama (z. B. Llama 3, Mistral) |

---

## âš™ï¸ Installation
1. **Ollama installieren** â†’ https://ollama.com/download  
2. **Modell laden** (Beispiel):
   ```bash
   ollama pull llama3
